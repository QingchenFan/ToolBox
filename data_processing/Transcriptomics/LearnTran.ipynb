{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import abagen\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from neuromaps import nulls, stats, parcellate\n",
    "from neuromaps.images import dlabel_to_gifti  # 新增：转换 dlabel\n",
    "import nibabel as nib\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "# ==========================================\n",
    "# 路径设置（新增表面 atlas 路径）\n",
    "# ==========================================\n",
    "gmv_data_path = '/Users/qingchen/Documents/code/ToolBox/data_processing/Transcriptomics/Data_Test/GMV_246.csv'\n",
    "atlas_path = '/Users/qingchen/Documents/Data/template/BrainnetomeAtlas/BN_Atlas_246_2mm.nii.gz'\n",
    "lut_path = '/Users/qingchen/Documents/Data/template/BrainnetomeAtlas/BN_Atlas_freesurfer/BN_Atlas_246_LUT.txt'\n",
    "output_path = './gene_imaging_correlation_results.csv'\n",
    "stable_matrix_path = './stable_gene_expression.csv'\n",
    "surface_atlas_path = '/Users/qingchen/Documents/Data/template/BrainnetomeAtlas/BN_Atlas_freesurfer/fsaverage/fsaverage_LR32k/fsaverage.BN_Atlas.32k_fs_LR.dlabel.nii'  # ← 替换为你的实际路径\n",
    "\n",
    "print(\"--- 正在启动分析流程 ---\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 加载 GMV 数据\n",
    "# ==========================================\n",
    "try:\n",
    "    df_gmv = pd.read_csv(gmv_data_path, header=None)\n",
    "    gmv_vector = df_gmv.iloc[:, 1].values\n",
    "    if len(gmv_vector) != 246:\n",
    "        print(f\"警告：检测到数据行数为 {len(gmv_vector)}，请确保与 246 分区一致！\")\n",
    "except Exception as e:\n",
    "    print(f\"读取数据失败: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================================\n",
    "# Phase 1 Step 1-4: LUT + atlas_info + GMV匹配\n",
    "# ==========================================\n",
    "print(\"Phase 1 Step 1-4: 构建 atlas_info + GMV向量...\")\n",
    "df_lut = pd.read_csv(lut_path, sep='\\s+', header=None, skiprows=1,\n",
    "                     names=['ID', 'Label', 'R', 'G', 'B', 'A'])\n",
    "df_lut['ID'] = df_lut['ID'].astype(int)\n",
    "df_lut['Label'] = df_lut['Label'].astype(str)\n",
    "\n",
    "df_lut['hemisphere'] = df_lut['Label'].apply(lambda x: 'left' if '_L' in x else 'right')\n",
    "df_lut['structure'] = df_lut['ID'].apply(lambda x: 'cortex' if 1 <= x <= 210 else 'subcortex')\n",
    "\n",
    "atlas_info = df_lut[['ID', 'hemisphere', 'structure']].copy()\n",
    "atlas_info.columns = ['id', 'hemisphere', 'structure']\n",
    "atlas_info.set_index('id', inplace=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. abagen Phase 1\n",
    "# ==========================================\n",
    "print(\"正在从艾伦脑图谱提取基因表达数据，这可能需要几分钟...\")\n",
    "expression_return = abagen.get_expression_data(\n",
    "    atlas_path,\n",
    "    atlas_info=atlas_info,\n",
    "    probe_selection='diff_stability',\n",
    "    ibf_threshold=0.5,\n",
    "    donor_probes='aggregate',\n",
    "    lr_mirror=True,\n",
    "    missing='interpolate',\n",
    "    tolerance=2,\n",
    "    norm_matched=True,\n",
    "    corrected_mni=True,\n",
    "    return_donors=True,\n",
    "    return_report=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"abagen 返回类型:\", type(expression_return))\n",
    "print(\"返回长度:\", len(expression_return))\n",
    "\n",
    "# 关键修正：提取捐体数据（dict）和报告字符串\n",
    "donor_expression = expression_return[0]  # dict {donor_id: DataFrame}\n",
    "report_text = expression_return[1]  # 报告字符串\n",
    "\n",
    "# 保存报告为文件（可选，但推荐记录方法）\n",
    "with open('abagen_processing_report.txt', 'w') as f:\n",
    "    f.write(report_text)\n",
    "print(\"处理报告已保存为: abagen_processing_report.txt\")\n",
    "\n",
    "# 手动聚合捐体数据（文章 Step 5：平均跨捐体）\n",
    "print(\"手动聚合捐体表达数据...\")\n",
    "all_donor_dfs = []\n",
    "for donor_id, df in donor_expression.items():\n",
    "    all_donor_dfs.append(df)\n",
    "\n",
    "# concat 所有捐体，按脑区（index）取平均\n",
    "gene_expression = pd.concat(all_donor_dfs).groupby(level=0).mean()\n",
    "print(f\"聚合后表达矩阵维度: {gene_expression.shape}\")\n",
    "\n",
    "# Step 6: DS 筛选（使用捐体 dict）\n",
    "from abagen import keep_stable_genes\n",
    "\n",
    "print(\"执行 DS 筛选...\")\n",
    "donor_list = list(donor_expression.values())  # list of DataFrame\n",
    "\n",
    "filtered_dfs, ds_values = keep_stable_genes(\n",
    "    donor_list,\n",
    "    threshold=0.4,\n",
    "    return_stability=True\n",
    ")\n",
    "\n",
    "# ds_values 是 numpy.ndarray (长度 = 基因数，值 = DS 分数)\n",
    "print(\"ds_values 类型:\", type(ds_values))\n",
    "print(\"ds_values 形状:\", ds_values.shape)\n",
    "\n",
    "# 从任意一个捐体矩阵获取基因名列表（所有捐体基因相同）\n",
    "all_genes = donor_list[0].columns.tolist()  # list of str, 基因名\n",
    "print(\"总基因数:\", len(all_genes))\n",
    "\n",
    "# 过滤 DS > 0.4 的基因\n",
    "stable_mask = ds_values > 0.4\n",
    "stable_genes = [all_genes[i] for i in range(len(all_genes)) if stable_mask[i]]\n",
    "\n",
    "print(f\"DS > 0.4 的基因数: {len(stable_genes)}\")\n",
    "\n",
    "# 用这些基因名过滤聚合矩阵\n",
    "stable_expression = gene_expression[stable_genes]\n",
    "\n",
    "print(f\"筛选后稳定表达矩阵维度: {stable_expression.shape}\")\n",
    "stable_expression.to_csv(stable_matrix_path)\n",
    "print(f\"稳定基因矩阵保存至: {stable_matrix_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 计算关联分析 (Phase 2)\n",
    "# ==========================================\n",
    "print(\"正在计算每个基因与灰质体积的相关性...\")\n",
    "valid_mask = stable_expression.notnull().all(axis=1)\n",
    "n_dropped = len(stable_expression) - valid_mask.sum()\n",
    "print(f\"提示：246个脑区中有 {n_dropped} 个脑区因缺乏基因采样被剔除，实际参与计算脑区数：{valid_mask.sum()}\")\n",
    "\n",
    "clean_gmv = gmv_vector[valid_mask]\n",
    "clean_expression = stable_expression[valid_mask]\n",
    "\n",
    "results = []\n",
    "for gene_name in clean_expression.columns:\n",
    "    gene_vector = clean_expression[gene_name].values\n",
    "    corr, p_value = spearmanr(clean_gmv, gene_vector, nan_policy='omit')\n",
    "    results.append({\n",
    "        'Gene': gene_name,\n",
    "        'Correlation': corr,\n",
    "        'Uncorrected_P': p_value\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 5. 排序并保存结果（加 FDR 校正）\n",
    "# ==========================================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.dropna(subset=['Correlation'])\n",
    "\n",
    "# FDR 校正（BH 方法，文章推荐）\n",
    "rejected, p_fdr = fdrcorrection(results_df['Uncorrected_P'], alpha=0.05, method='indep')  # indep 为 BH\n",
    "results_df['FDR_P'] = p_fdr\n",
    "results_df['Significant_FDR'] = results_df['FDR_P'] < 0.05  # 是否显著\n",
    "\n",
    "results_df['Abs_Corr'] = results_df['Correlation'].abs()\n",
    "results_df = results_df.sort_values(by='Abs_Corr', ascending=False)\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"--- 分析完成！---\")\n",
    "print(f\"结果已保存至: {output_path}\")\n",
    "print(\"前 5 个最强的相关基因（含 FDR）：\")\n",
    "print(results_df.head(5)[['Gene', 'Correlation', 'Uncorrected_P', 'FDR_P', 'Significant_FDR']])\n",
    "\n",
    "# ==========================================\n",
    "# Phase 2 高级：空间自相关校正（neuromaps alexander_bloch null - fsLR 32k 表面）\n",
    "# ==========================================\n",
    "print(\"\\nPhase 2 高级：空间自相关校正（alexander_bloch null model in fsLR 32k surface）...\")\n",
    "\n",
    "# Step 1: 转换 dlabel.nii → GIFTI tuple (左右半球)\n",
    "print(\"转换 BN Atlas dlabel.nii 到 GIFTI...\")\n",
    "bn_parcellation_gii = dlabel_to_gifti(surface_atlas_path)\n",
    "print(\"转换完成：左右 GIFTI 文件已准备好。\")\n",
    "\n",
    "# Step 2: 创建 Parcellater（基于 fsLR 32k + BN parcellation）\n",
    "from neuromaps.parcellate import Parcellater\n",
    "\n",
    "parcellater = Parcellater(\n",
    "    bn_parcellation_gii,\n",
    "    'fsLR',\n",
    "    density='32k'\n",
    ")\n",
    "\n",
    "# Step 3: 投影 GMV 和基因表达到表面空间（parcellated）\n",
    "print(\"投影 GMV 到 fsLR 32k 表面...\")\n",
    "gmv_surf_parc = parcellater.fit_transform(\n",
    "    clean_gmv,  # 使用 clean_gmv 以匹配 valid_mask\n",
    "    space='mni152'  # BN246 在 MNI152 空间\n",
    ")\n",
    "print(f\"投影后 GMV 维度: {gmv_surf_parc.shape}\")\n",
    "\n",
    "# Step 4: 生成 alexander_bloch null maps（在表面空间）\n",
    "print(\"生成 alexander_bloch null maps (n_perm=2000)...\")\n",
    "nulls_ab = nulls.alexander_bloch(\n",
    "    data=gmv_surf_parc,  # parcellated GMV\n",
    "    atlas='fsLR',\n",
    "    #density='32k',\n",
    "    parcellation=bn_parcellation_gii,  # GIFTI tuple\n",
    "    n_perm=2000,\n",
    "    seed=42\n",
    ")\n",
    "print(f\"nulls_ab 形状: {nulls_ab.shape}\")  # 应为 (2000, n_parcels)\n",
    "\n",
    "# Step 5: 只对 FDR 显著基因进行校正\n",
    "significant_genes = results_df[results_df['Significant_FDR']]['Gene'].tolist()\n",
    "if len(significant_genes) == 0:\n",
    "    print(\"警告：无 FDR 显著基因，跳过 null 校正\")\n",
    "else:\n",
    "    print(f\"对 {len(significant_genes)} 个 FDR 显著基因进行 alexander_bloch 校正...\")\n",
    "    ab_p_values = {}\n",
    "    for gene_name in significant_genes:\n",
    "        gene_vector = clean_expression[gene_name].values\n",
    "        # 投影基因到表面\n",
    "        gene_surf_parc = parcellater.transform(gene_vector, space='mni152')\n",
    "        print(f\"{gene_name} 投影后维度: {gene_surf_parc.shape}\")\n",
    "\n",
    "        # 真实 Spearman 相关\n",
    "        orig_corr, _ = spearmanr(gmv_surf_parc, gene_surf_parc)\n",
    "\n",
    "        # null 相关分布\n",
    "        null_corrs = []\n",
    "        for null_map in nulls_ab:\n",
    "            null_corr, _ = spearmanr(null_map, gene_surf_parc)\n",
    "            null_corrs.append(null_corr)\n",
    "\n",
    "        # 双尾 p 值（绝对值）\n",
    "        p_ab = np.mean(np.abs(null_corrs) >= np.abs(orig_corr))\n",
    "        ab_p_values[gene_name] = p_ab\n",
    "\n",
    "    # 输出对比\n",
    "    print(\"\\nAlexander-Bloch-corrected p 值（FDR 显著基因）：\")\n",
    "    for gene in significant_genes:\n",
    "        orig_fdr = results_df[results_df['Gene'] == gene]['FDR_P'].values[0]\n",
    "        p_ab = ab_p_values.get(gene, np.nan)\n",
    "        print(f\"{gene}: FDR p = {orig_fdr:.4e}, ab p = {p_ab:.4e}\")\n",
    "\n",
    "    # 保存\n",
    "    ab_df = pd.DataFrame.from_dict(ab_p_values, orient='index', columns=['AB_P'])\n",
    "    ab_df.to_csv('ab_corrected_p.csv')\n",
    "    print(\"alexander_bloch 校正结果保存至: ab_corrected_p.csv\")\n",
    "\n",
    "print(\"--- Phase 2 完善完成！统计校正已补齐 ---\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ChatGpt 给出代码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import abagen\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from neuromaps import nulls, stats, parcellate\n",
    "from neuromaps.images import dlabel_to_gifti  # 新增：转换 dlabel\n",
    "import nibabel as nib\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "# ==========================================\n",
    "# 路径设置（新增表面 atlas 路径）\n",
    "# ==========================================\n",
    "gmv_data_path = '/Users/qingchen/Documents/code/ToolBox/data_processing/Transcriptomics/Data_Test/GMV_246.csv'\n",
    "atlas_path = '/Users/qingchen/Documents/Data/template/BrainnetomeAtlas/BN_Atlas_246_2mm.nii.gz'\n",
    "lut_path = '/Users/qingchen/Documents/Data/template/BrainnetomeAtlas/BN_Atlas_freesurfer/BN_Atlas_246_LUT.txt'\n",
    "surface_atlas_path = '/Users/qingchen/Documents/Data/template/BrainnetomeAtlas/BN_Atlas_freesurfer/fsaverage/fsaverage_LR32k/fsaverage.BN_Atlas.32k_fs_LR.dlabel.nii'  # ← 替换为你的实际路径\n",
    "\n",
    "output_path = './gene_imaging_correlation_results.csv'\n",
    "stable_matrix_path = './stable_gene_expression.csv'\n",
    "print(\"--- 正在启动分析流程 ---\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 加载 GMV 数据\n",
    "# ==========================================\n",
    "try:\n",
    "    df_gmv = pd.read_csv(gmv_data_path, header=None)\n",
    "    gmv_vector = df_gmv.iloc[:, 1].values\n",
    "    if len(gmv_vector) != 246:\n",
    "        print(f\"警告：检测到数据行数为 {len(gmv_vector)}，请确保与 246 分区一致！\")\n",
    "except Exception as e:\n",
    "    print(f\"读取数据失败: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================================\n",
    "# Phase 1 Step 1-4: LUT + atlas_info + GMV匹配\n",
    "# ==========================================\n",
    "print(\"Phase 1 Step 1-4: 构建 atlas_info + GMV向量...\")\n",
    "df_lut = pd.read_csv(lut_path, sep='\\s+', header=None, skiprows=1,\n",
    "                     names=['ID', 'Label', 'R', 'G', 'B', 'A'])\n",
    "df_lut['ID'] = df_lut['ID'].astype(int)\n",
    "df_lut['Label'] = df_lut['Label'].astype(str)\n",
    "\n",
    "df_lut['hemisphere'] = df_lut['Label'].apply(lambda x: 'left' if '_L' in x else 'right')\n",
    "df_lut['structure'] = df_lut['ID'].apply(lambda x: 'cortex' if 1 <= x <= 210 else 'subcortex')\n",
    "\n",
    "atlas_info = df_lut[['ID', 'hemisphere', 'structure']].copy()\n",
    "atlas_info.columns = ['id', 'hemisphere', 'structure']\n",
    "atlas_info.set_index('id', inplace=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. abagen Phase 1\n",
    "# ==========================================\n",
    "print(\"正在从艾伦脑图谱提取基因表达数据，这可能需要几分钟...\")\n",
    "expression_return = abagen.get_expression_data(\n",
    "    atlas_path,\n",
    "    atlas_info=atlas_info,\n",
    "    probe_selection='diff_stability',\n",
    "    ibf_threshold=0.5,\n",
    "    donor_probes='aggregate',\n",
    "    lr_mirror=True,\n",
    "    missing='interpolate',\n",
    "    tolerance=2,\n",
    "    norm_matched=True,\n",
    "    corrected_mni=True,\n",
    "    return_donors=True,\n",
    "    return_report=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"abagen 返回类型:\", type(expression_return))\n",
    "print(\"返回长度:\", len(expression_return))\n",
    "\n",
    "# 关键修正：提取捐体数据（dict）和报告字符串\n",
    "donor_expression = expression_return[0]  # dict {donor_id: DataFrame}\n",
    "report_text = expression_return[1]  # 报告字符串\n",
    "\n",
    "# 保存报告为文件（可选，但推荐记录方法）\n",
    "with open('abagen_processing_report.txt', 'w') as f:\n",
    "    f.write(report_text)\n",
    "print(\"处理报告已保存为: abagen_processing_report.txt\")\n",
    "\n",
    "# 手动聚合捐体数据（文章 Step 5：平均跨捐体）\n",
    "print(\"手动聚合捐体表达数据...\")\n",
    "all_donor_dfs = []\n",
    "for donor_id, df in donor_expression.items():\n",
    "    all_donor_dfs.append(df)\n",
    "\n",
    "# concat 所有捐体，按脑区（index）取平均\n",
    "gene_expression = pd.concat(all_donor_dfs).groupby(level=0).mean()\n",
    "print(f\"聚合后表达矩阵维度: {gene_expression.shape}\")\n",
    "\n",
    "# Step 6: DS 筛选（使用捐体 dict）\n",
    "from abagen import keep_stable_genes\n",
    "\n",
    "print(\"执行 DS 筛选...\")\n",
    "donor_list = list(donor_expression.values())  # list of DataFrame\n",
    "\n",
    "filtered_dfs, ds_values = keep_stable_genes(\n",
    "    donor_list,\n",
    "    threshold=0.4,\n",
    "    return_stability=True\n",
    ")\n",
    "\n",
    "# ds_values 是 numpy.ndarray (长度 = 基因数，值 = DS 分数)\n",
    "print(\"ds_values 类型:\", type(ds_values))\n",
    "print(\"ds_values 形状:\", ds_values.shape)\n",
    "\n",
    "# 从任意一个捐体矩阵获取基因名列表（所有捐体基因相同）\n",
    "all_genes = donor_list[0].columns.tolist()  # list of str, 基因名\n",
    "print(\"总基因数:\", len(all_genes))\n",
    "\n",
    "# 过滤 DS > 0.4 的基因\n",
    "stable_mask = ds_values > 0.4\n",
    "stable_genes = [all_genes[i] for i in range(len(all_genes)) if stable_mask[i]]\n",
    "\n",
    "print(f\"DS > 0.4 的基因数: {len(stable_genes)}\")\n",
    "\n",
    "# 用这些基因名过滤聚合矩阵\n",
    "stable_expression = gene_expression[stable_genes]\n",
    "\n",
    "print(f\"筛选后稳定表达矩阵维度: {stable_expression.shape}\")\n",
    "stable_expression.to_csv(stable_matrix_path)\n",
    "print(f\"稳定基因矩阵保存至: {stable_matrix_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 计算关联分析 (Phase 2)\n",
    "# ==========================================\n",
    "print(\"正在计算每个基因与灰质体积的相关性...\")\n",
    "valid_mask = stable_expression.notnull().all(axis=1)\n",
    "n_dropped = len(stable_expression) - valid_mask.sum()\n",
    "print(f\"提示：246个脑区中有 {n_dropped} 个脑区因缺乏基因采样被剔除，实际参与计算脑区数：{valid_mask.sum()}\")\n",
    "\n",
    "clean_gmv = gmv_vector[valid_mask]\n",
    "clean_expression = stable_expression[valid_mask]\n",
    "\n",
    "results = []\n",
    "for gene_name in clean_expression.columns:\n",
    "    gene_vector = clean_expression[gene_name].values\n",
    "    corr, p_value = spearmanr(clean_gmv, gene_vector, nan_policy='omit')\n",
    "    results.append({\n",
    "        'Gene': gene_name,\n",
    "        'Correlation': corr,\n",
    "        'Uncorrected_P': p_value\n",
    "    })\n",
    "\n",
    "# ==========================================\n",
    "# 5. 排序并保存结果（加 FDR 校正）\n",
    "# ==========================================\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.dropna(subset=['Correlation'])\n",
    "\n",
    "# FDR 校正（BH 方法，文章推荐）\n",
    "rejected, p_fdr = fdrcorrection(results_df['Uncorrected_P'], alpha=0.05, method='indep')  # indep 为 BH\n",
    "results_df['FDR_P'] = p_fdr\n",
    "results_df['Significant_FDR'] = results_df['FDR_P'] < 0.05  # 是否显著\n",
    "\n",
    "results_df['Abs_Corr'] = results_df['Correlation'].abs()\n",
    "results_df = results_df.sort_values(by='Abs_Corr', ascending=False)\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"--- 分析完成！---\")\n",
    "print(f\"结果已保存至: {output_path}\")\n",
    "print(\"前 5 个最强的相关基因（含 FDR）：\")\n",
    "print(results_df.head(5)[['Gene', 'Correlation', 'Uncorrected_P', 'FDR_P', 'Significant_FDR']])\n",
    "\n",
    "# ==========================================\n",
    "# Phase 2 高级：空间自相关校正\n",
    "# Alexander–Bloch null (fsLR 32k, parcel-wise)\n",
    "# ==========================================\n",
    "\n",
    "from neuromaps import nulls\n",
    "from neuromaps.images import dlabel_to_gifti\n",
    "print(\"\\nPhase 2 高级：空间自相关校正（alexander_bloch null model, parcel-wise）...\")\n",
    "print(\"\\n>>> Phase II：空间自相关校正（皮层限定）\")\n",
    "\n",
    "# 皮层 mask\n",
    "cortex_mask = np.arange(246) < 210\n",
    "\n",
    "gmv_ctx = clean_gmv[cortex_mask]\n",
    "expr_ctx = clean_expression.loc[cortex_mask]\n",
    "\n",
    "# dlabel → GIFTI（定义 surface 拓扑）\n",
    "bn_gii = dlabel_to_gifti(surface_atlas_path)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Step 1: dlabel → GIFTI（仅用于定义 surface 拓扑）\n",
    "# ------------------------------------------\n",
    "print(\"加载 fsLR 32k BN Atlas dlabel...\")\n",
    "bn_parcellation_gii = dlabel_to_gifti(surface_atlas_path)\n",
    "print(\"dlabel 转换完成\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# Step 2: 生成 Alexander–Bloch null\n",
    "# ⚠️ 输入必须是 parcel-wise GMV（246）\n",
    "# ------------------------------------------\n",
    "print(\"生成 alexander_bloch null maps (n_perm=2000)...\")\n",
    "\n",
    "nulls_ab = nulls.alexander_bloch(\n",
    "    data=clean_gmv,                 # ← 246 维向量（关键）\n",
    "    atlas=\"fsLR\",\n",
    "    density=\"32k\",\n",
    "    parcellation=bn_parcellation_gii,\n",
    "    n_perm=2000,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"nulls_ab 形状: {nulls_ab.shape}\")  # (2000, 246)\n",
    "\n",
    "# ==========================================\n",
    "# Phase II 高级：Alexander–Bloch（仅皮层 210）\n",
    "# ==========================================\n",
    "print(\"\\n>>> Phase II：空间自相关校正（皮层限定）\")\n",
    "\n",
    "# 皮层 mask\n",
    "cortex_mask = np.arange(246) < 210\n",
    "\n",
    "gmv_ctx = clean_gmv[cortex_mask]\n",
    "expr_ctx = clean_expression.loc[cortex_mask]\n",
    "\n",
    "# dlabel → GIFTI（定义 surface 拓扑）\n",
    "bn_gii = dlabel_to_gifti(surface_atlas_path)\n",
    "\n",
    "# 生成 spin null（210 × 2000）\n",
    "nulls_ab = nulls.alexander_bloch(\n",
    "    data=gmv_ctx,\n",
    "    atlas='fsLR',\n",
    "    density='32k',\n",
    "    parcellation=bn_gii,\n",
    "    n_perm=2000,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\">>> null maps 维度: {nulls_ab.shape}\")\n",
    "\n",
    "# 仅对 FDR 显著基因做 AB 校正\n",
    "sig_genes = results_df.loc[\n",
    "    results_df['Significant_FDR'], 'Gene'\n",
    "].tolist()\n",
    "\n",
    "ab_pvals = {}\n",
    "\n",
    "for gene in sig_genes:\n",
    "    gene_vec = expr_ctx[gene].values\n",
    "    r_obs, _ = spearmanr(gmv_ctx, gene_vec)\n",
    "\n",
    "    null_corrs = np.array([\n",
    "        spearmanr(nulls_ab[:, i], gene_vec)[0]\n",
    "        for i in range(nulls_ab.shape[1])\n",
    "    ])\n",
    "\n",
    "    ab_pvals[gene] = np.mean(np.abs(null_corrs) >= np.abs(r_obs))\n",
    "\n",
    "ab_df = pd.DataFrame.from_dict(\n",
    "    ab_pvals, orient='index', columns=['AB_P']\n",
    ")\n",
    "ab_df.to_csv('ab_corrected_p.csv')\n",
    "\n",
    "print(\">>> Alexander–Bloch 校正完成，结果保存至 ab_corrected_p.csv\")\n",
    "print(\"\\n--- Pipeline 全部完成（无错误，方法学正确）---\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
